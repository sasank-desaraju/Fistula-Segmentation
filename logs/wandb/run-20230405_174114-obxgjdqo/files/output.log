
{'PROJECT_NAME': 'FistulaSegmentation', 'MODEL_NAME': 'Development', 'RUN_NAME': '2023-04-05-17-41-13', 'WANDB_RUN_GROUP': 'Local', 'FAST_DEV_RUN': True, 'MAX_EPOCHS': 1, 'MAX_STEPS': 5, 'STRATEGY': None, 'DATA_DIR': 'data', 'RANDOM_STATE': 42, 'DATA_NAME': 'BaseSplit', 'USE_TRANSFORMS': False, 'IMAGE_ROOT': '/media/sasank/LinuxStorage/Dropbox (UFL)/FistulaData/Segmentations/', 'IMAGE_SIZE': [512, 512, 96], 'CKPT_FILE': '', 'BATCH_SIZE': 1, 'SHUFFLE': True, 'NUM_WORKERS': 2, 'PIN_MEMORY': False, 'LOAD_FROM_CHECKPOINT': False, 'learning_rate': 0.001}
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.
Epoch 0:   0%|                                                                                                                                                                                     | 0/2 [00:00<?, ?it/s]
Checkpoint directory /home/sasank/Documents/GitRepos/Fistula-Segmentation/checkpoints exists and is not empty.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
  | Name          | Type     | Params
-------------------------------------------
0 | _model        | UNet     | 4.8 M
1 | loss_function | DiceLoss | 0
-------------------------------------------
4.8 M     Trainable params
0         Non-trainable params
4.8 M     Total params
19.232    Total estimated model params size (MB)
The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=5). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                              | 1/2 [00:07<00:07,  7.35s/it, loss=1, v_num=]
Validation: 0it [00:00, ?it/s]

y_pred should be a binarized tensor.
`Trainer.fit` stopped: `max_steps=1` reached.
[34m[1mwandb[39m[22m: [33mWARNING[39m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")